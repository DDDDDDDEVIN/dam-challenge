{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/curated/cleaned_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_df = pd.get_dummies(df, columns=['region', 'primary_purpose', 'primary_type', 'spillway', 'assessment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_df['hazard'] = encoded_df['hazard'].replace({\n",
    "    'Low': 1,\n",
    "    'High': 2,\n",
    "    'Significant': 3,\n",
    "    'Undetermined': 0\n",
    "})\n",
    "\n",
    "encoded_df['regulated_dam'] = encoded_df['hazard'].replace({\n",
    "    'Yes': 1,\n",
    "    'No': 0\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take log transformation\n",
    "for feature in ['height', 'volume', 'drainage', 'length', 'surface']:\n",
    "    encoded_df[feature] = encoded_df[feature].apply(lambda x: np.log(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = MinMaxScaler()\n",
    "feature_df = encoded_df.drop(columns=['dam_repair_loss', 'damage_loss', 'business_interruption_loss', 'probability_of_failure'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_df[feature_df.columns] = pd.DataFrame(min_max_scaler.fit_transform(feature_df), \n",
    "                                                                                columns=feature_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalised_df = encoded_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Val Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into null, train, val, test set, with 80%, 10%, 10% distribution\n",
    "def null_train_val_test_split(df, label):\n",
    "    # split null rows out of df\n",
    "    null_df = df.loc[df[label].isna()]\n",
    "    temp_df = df.loc[df[label].notna()]\n",
    "    # split traindf\n",
    "    train_df, val_test_df = train_test_split(temp_df, test_size=0.2, random_state=42)\n",
    "    # split val and test df\n",
    "    val_df, test_df = train_test_split(val_test_df, test_size=0.5, random_state=42)\n",
    "    return null_df, train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solit data set into feature and label parts\n",
    "def X_Y_split(df, label):\n",
    "    X = df.drop(columns=label)\n",
    "    Y = df[label]\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dam Repair Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "repair_df = normalised_df.drop(columns=['damage_loss', 'business_interruption_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "repair_null_df, repair_train_df, repair_val_df, repair_test_df = null_train_val_test_split(df=repair_df, label='dam_repair_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "repair_train_X, repair_train_Y = X_Y_split(repair_train_df, 'dam_repair_loss')\n",
    "repair_val_X, repair_val_Y = X_Y_split(repair_val_df, 'dam_repair_loss')\n",
    "repair_test_X, repair_test_Y = X_Y_split(repair_test_df, 'dam_repair_loss')\n",
    "repair_null_X, _ = X_Y_split(repair_null_df, 'dam_repair_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13584    693.8\n",
       "7757      34.3\n",
       "8184       5.5\n",
       "5938      12.2\n",
       "2319      12.6\n",
       "         ...  \n",
       "18908    363.5\n",
       "12120     47.5\n",
       "10682     42.3\n",
       "1219      16.5\n",
       "4476      13.7\n",
       "Name: dam_repair_loss, Length: 2080, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repair_val_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "repair_null_X.to_csv('../data/curated/repair/null_X.csv', index=False)\n",
    "repair_train_X.to_csv('../data/curated/repair/train_X.csv', index=False)\n",
    "repair_train_Y.to_csv('../data/curated/repair/train_Y.csv', index=False)\n",
    "repair_val_X.to_csv('../data/curated/repair/val_X.csv', index=False)\n",
    "repair_val_Y.to_csv('../data/curated/repair/val_Y.csv', index=False)\n",
    "repair_test_X.to_csv('../data/curated/repair/test_X.csv', index=False)\n",
    "repair_test_Y.to_csv('../data/curated/repair/test_Y.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Damage Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "damage_df = normalised_df.drop(columns=['dam_repair_loss', 'business_interruption_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "damage_null_df, damage_train_df, damage_val_df, damage_test_df = null_train_val_test_split(df=damage_df, label='damage_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "damage_train_X, damage_train_Y = X_Y_split(damage_train_df, 'damage_loss')\n",
    "damage_val_X, damage_val_Y = X_Y_split(damage_val_df, 'damage_loss')\n",
    "damage_test_X, damage_test_Y = X_Y_split(damage_test_df, 'damage_loss')\n",
    "damage_null_X, _ = X_Y_split(damage_null_df, 'damage_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14968    184.1\n",
       "5233       9.7\n",
       "16018    645.2\n",
       "47       863.9\n",
       "11081    204.3\n",
       "         ...  \n",
       "11293    421.6\n",
       "11974     18.0\n",
       "5396      13.9\n",
       "860       19.1\n",
       "15805     23.2\n",
       "Name: damage_loss, Length: 16635, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "damage_train_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "damage_null_X.to_csv('../data/curated/damage/null_X.csv', index=False)\n",
    "damage_train_X.to_csv('../data/curated/damage/train_X.csv', index=False)\n",
    "damage_train_Y.to_csv('../data/curated/damage/train_Y.csv', index=False)\n",
    "damage_val_X.to_csv('../data/curated/damage/val_X.csv', index=False)\n",
    "damage_val_Y.to_csv('../data/curated/damage/val_Y.csv', index=False)\n",
    "damage_test_X.to_csv('../data/curated/damage/test_X.csv', index=False)\n",
    "damage_test_Y.to_csv('../data/curated/damage/test_Y.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Business Interruption Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "BI_df = normalised_df.drop(columns=['damage_loss', 'dam_repair_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "BI_null_df, BI_train_df, BI_val_df, BI_test_df = null_train_val_test_split(df=BI_df, label='business_interruption_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "BI_train_X, BI_train_Y = X_Y_split(BI_train_df, 'business_interruption_loss')\n",
    "BI_val_X, BI_val_Y = X_Y_split(BI_val_df, 'business_interruption_loss')\n",
    "BI_test_X, BI_test_Y = X_Y_split(BI_test_df, 'business_interruption_loss')\n",
    "BI_null_X, _ = X_Y_split(BI_null_df, 'business_interruption_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "BI_null_X.to_csv('../data/curated/business_interruption/null_X.csv', index=False)\n",
    "BI_train_X.to_csv('../data/curated/business_interruption/train_X.csv', index=False)\n",
    "BI_train_Y.to_csv('../data/curated/business_interruption/train_Y.csv', index=False)\n",
    "BI_val_X.to_csv('../data/curated/business_interruption/val_X.csv', index=False)\n",
    "BI_val_Y.to_csv('../data/curated/business_interruption/val_Y.csv', index=False)\n",
    "BI_test_X.to_csv('../data/curated/business_interruption/test_X.csv', index=False)\n",
    "BI_test_Y.to_csv('../data/curated/business_interruption/test_Y.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probabilty of Failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "PF_df = normalised_df.drop(columns=['business_interruption_loss', 'damage_loss', 'dam_repair_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "PF_null_df, PF_train_df, PF_val_df, PF_test_df = null_train_val_test_split(df=PF_df, label='probability_of_failure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "PF_train_X, PF_train_Y = X_Y_split(PF_train_df, 'probability_of_failure')\n",
    "PF_val_X, PF_val_Y = X_Y_split(PF_val_df, 'probability_of_failure')\n",
    "PF_test_X, PF_test_Y = X_Y_split(PF_test_df, 'probability_of_failure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "PF_train_X.to_csv('../data/curated/probability_of_failure/train_X.csv', index=False)\n",
    "PF_train_Y.to_csv('../data/curated/probability_of_failure/train_Y.csv', index=False)\n",
    "PF_val_X.to_csv('../data/curated/probability_of_failure/val_X.csv', index=False)\n",
    "PF_val_Y.to_csv('../data/curated/probability_of_failure/val_Y.csv', index=False)\n",
    "PF_test_X.to_csv('../data/curated/probability_of_failure/test_X.csv', index=False)\n",
    "PF_test_Y.to_csv('../data/curated/probability_of_failure/test_Y.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dam Repair Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rfe_selection(estimator):\n",
    "    rfe = RFE(estimator=estimator)\n",
    "    rfe."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
